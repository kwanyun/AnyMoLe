{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Ego4D FHO Clips Using PyTorchVideo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load `fho_main.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../ego4d/v2/annotations/fho_main.json\") as f:\n",
    "    fho_main = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick an arbitrary action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = fho_main[\"videos\"][20]\n",
    "interval = video[\"annotated_intervals\"][2]\n",
    "action = interval[\"narrated_actions\"][4]\n",
    "\n",
    "print(f'video_uid: {video[\"video_uid\"]}')\n",
    "print(f'start_sec: {action[\"start_sec\"]}')\n",
    "print(f'end_sec: {action[\"end_sec\"]}')\n",
    "print(f'clip_uid: {interval[\"clip_uid\"]}')\n",
    "print(f'clip_start_sec: {action[\"clip_start_sec\"]}')\n",
    "print(f'clip_end_sec: {action[\"clip_end_sec\"]}')\n",
    "print(f'narration_text: {action[\"narration_text\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the action from the full video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def display_video(kind, uid, start_sec, end_sec):\n",
    "    html = f\"\"\"\n",
    "        <video id=\"video-{kind}-{uid}\" width=\"480\" height=\"320\" controls>\n",
    "            <source src=\"../../ego4d/v2/{kind}/{uid}.mp4\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        <script>\n",
    "            var video = document.getElementById('video-{kind}-{uid}');\n",
    "            video.currentTime = {start_sec};\n",
    "            video.addEventListener('timeupdate', function() {{\n",
    "                if (video.currentTime >= {end_sec}) {{\n",
    "                    video.pause();\n",
    "                }}\n",
    "            }});\n",
    "        </script>\n",
    "        \"\"\"\n",
    "\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "display_video(\"full_scale\", video[\"video_uid\"], action[\"start_sec\"], action[\"end_sec\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the action from the clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(\n",
    "    \"clips\", interval[\"clip_uid\"], action[\"clip_start_sec\"], action[\"clip_end_sec\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both mark the same action, so we can just use the clips, which are smaller and more wieldy.\n",
    "\n",
    "Now let's load the clip, and extract the frames corresponding to the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "\n",
    "video_path_handler = VideoPathHandler()\n",
    "\n",
    "# First, load the video corresponding to the clip\n",
    "video = video_path_handler.video_from_path(\n",
    "    f\"../../ego4d/v2/clips/{interval['clip_uid']}.mp4\"\n",
    ")\n",
    "\n",
    "# Now extract the clip corresponding to the action\n",
    "clip = video.get_clip(action[\"clip_start_sec\"], action[\"clip_end_sec\"])\n",
    "\n",
    "# frame tensor for the action\n",
    "# the action is 8 seconds, and the clip is 30fps, so 240 frames are extracted.\n",
    "# (C, T, H, W)\n",
    "print(clip[\"video\"].size())\n",
    "\n",
    "# audio\n",
    "print(clip[\"audio\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to use every frame to save on computation. Let's uniformly subsample 8 frames across the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "\n",
    "subsampler = UniformTemporalSubsample(8)\n",
    "# (T, C, H, W)\n",
    "subsampled_frames = subsampler(clip[\"video\"]).permute(1, 0, 2, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the subsampled frames as a gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_file_name):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    iio.imwrite(\n",
    "        gif_file_name,\n",
    "        video_tensor.permute(0, 2, 3, 1).numpy().astype(np.uint8),\n",
    "        extension=\".gif\",\n",
    "        # infinite loop\n",
    "        loop=0,\n",
    "    )\n",
    "    html = f'<img src=\"{gif_file_name}\" />'\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "display_gif(subsampled_frames, \"subsampled_frames.gif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's preprocess the subsampled frames using `BlipImageProcessor`, and take a look as a gif. Note that the colors will look all wrong b/c they're normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "print(processor.image_processor)\n",
    "\n",
    "# treat the time dimension as the batch dimension to match Blip2Processor's expectation\n",
    "processed_frames = processor.image_processor(subsampled_frames, return_tensors=\"pt\")[\n",
    "    \"pixel_values\"\n",
    "]\n",
    "\n",
    "# (T, C, H, W)\n",
    "print(f\"processed_frames.size(): {processed_frames.size()}\")\n",
    "\n",
    "display_gif(processed_frames, \"processed_frames.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
