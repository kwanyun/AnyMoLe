{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load `fho_main.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../../ego4d/v2/annotations/fho_main.json\") as f:\n",
    "    fho_main = json.load(f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many structured verbs are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from eilev.data.ego4d import filter_action\n",
    "\n",
    "struct_verb_counter = Counter()\n",
    "freeform_verb_counter = Counter()\n",
    "no_critical_frames_frames = []\n",
    "critical_frames_no_frames = []\n",
    "non_other_struct_and_freeform_verb = 0\n",
    "for video in fho_main[\"videos\"]:\n",
    "    for interval in video[\"annotated_intervals\"]:\n",
    "        for action in interval[\"narrated_actions\"]:\n",
    "            if action[\"critical_frames\"] is None and action[\"frames\"] is not None:\n",
    "                no_critical_frames_frames.append(action)\n",
    "            elif action[\"critical_frames\"] is not None and action[\"frames\"] is None:\n",
    "                critical_frames_no_frames.append(action)\n",
    "            if filter_action(action):\n",
    "                if action[\"structured_verb\"] == \"[other]\":\n",
    "                    freeform_verb_counter[action[\"freeform_verb\"].strip().lower()] += 1\n",
    "                elif action[\"freeform_verb\"] is not None:\n",
    "                    non_other_struct_and_freeform_verb += 1\n",
    "                struct_verb_counter[action[\"structured_verb\"]] += 1\n",
    "\n",
    "print(f\"len(no_critical_frames_frames) = {len(no_critical_frames_frames)}\")\n",
    "print(f\"len(critical_frames_no_frames) = {len(critical_frames_no_frames)}\")\n",
    "print(f\"len(struct_verb_counter) = {len(struct_verb_counter)}\")\n",
    "print(f\"len(freeform_verb_counter) = {len(freeform_verb_counter)}\")\n",
    "print(f\"non_other_struct_and_freeform_verb = {non_other_struct_and_freeform_verb}\")\n",
    "print(\"=====structured verbs=========\")\n",
    "for verb, count in struct_verb_counter.items():\n",
    "    print(f\"{verb}: {count}\")\n",
    "print(\"=====freeform verbs=========\")\n",
    "for verb, count in freeform_verb_counter.items():\n",
    "    print(f\"{verb}: {count}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many structured nouns are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_noun_counter = Counter()\n",
    "freeform_noun_counter = Counter()\n",
    "none_struct_and_freeform_noun = 0\n",
    "for video in fho_main[\"videos\"]:\n",
    "    for interval in video[\"annotated_intervals\"]:\n",
    "        for action in interval[\"narrated_actions\"]:\n",
    "            if not filter_action(action):\n",
    "                continue\n",
    "            if action[\"frames\"] is None:\n",
    "                continue\n",
    "            for frame in action[\"frames\"]:\n",
    "                if frame[\"frame_type\"] != \"pnr_frame\":\n",
    "                    # some actions don't have contact frames so use pnr_frame\n",
    "                    continue\n",
    "                for box in frame[\"boxes\"]:\n",
    "                    if box[\"object_type\"] != \"object_of_change\":\n",
    "                        continue\n",
    "                    if box[\"structured_noun\"] is None:\n",
    "                        if box[\"freeform_noun\"] is not None:\n",
    "                            freeform_noun_counter[\n",
    "                                box[\"freeform_noun\"].strip().lower()\n",
    "                            ] += 1\n",
    "                        else:\n",
    "                            none_struct_and_freeform_noun += 1\n",
    "                    struct_noun_counter[box[\"structured_noun\"]] += 1\n",
    "\n",
    "print(f\"len(struct_noun_counter) = {len(struct_noun_counter)}\")\n",
    "print(f\"len(freeform_noun_counter) = {len(freeform_noun_counter)}\")\n",
    "print(f\"none_struct_and_freeform_noun = {none_struct_and_freeform_noun}\")\n",
    "print(\"=====structured nouns=========\")\n",
    "for noun, count in struct_noun_counter.items():\n",
    "    print(f\"{noun}: {count}\")\n",
    "print(\"=====freeform nouns=========\")\n",
    "for noun, count in freeform_noun_counter.items():\n",
    "    print(f\"{noun}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many (structured_verb, structured_noun)'s are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_verb_noun_counter = Counter()\n",
    "for video in fho_main[\"videos\"]:\n",
    "    for interval in video[\"annotated_intervals\"]:\n",
    "        for action in interval[\"narrated_actions\"]:\n",
    "            if not filter_action(action):\n",
    "                continue\n",
    "            verb = action[\"structured_verb\"]\n",
    "            if verb is None:\n",
    "                continue\n",
    "            if action[\"frames\"] is None:\n",
    "                continue\n",
    "            for frame in action[\"frames\"]:\n",
    "                if frame[\"frame_type\"] != \"pnr_frame\":\n",
    "                    # some actions don't have contact frames so use pnr_frame\n",
    "                    continue\n",
    "                for box in frame[\"boxes\"]:\n",
    "                    if box[\"object_type\"] != \"object_of_change\":\n",
    "                        continue\n",
    "                    if box[\"structured_noun\"] is None:\n",
    "                        # TODO: Maybe don't filter this out?\n",
    "                        continue\n",
    "                    noun = box[\"structured_noun\"]\n",
    "            struct_verb_noun_counter[(verb, noun)] += 1\n",
    "\n",
    "print(f\"len(struct_verb_noun_counter) = {len(struct_verb_noun_counter)}\")\n",
    "print(\"=====structured (verb, noun)s=========\")\n",
    "for (verb, noun), count in struct_verb_noun_counter.items():\n",
    "    print(f\"({verb}, {noun}): {count}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about in `fho_lta_taxonomy.json`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../ego4d/v2/annotations/fho_lta_taxonomy.json\") as f:\n",
    "    fho_lta_taxonomy = json.load(f)\n",
    "\n",
    "taxonomy_verbs = set(fho_lta_taxonomy[\"verbs\"])\n",
    "taxonomy_nouns = set(fho_lta_taxonomy[\"nouns\"])\n",
    "\n",
    "print(f\"len(taxonomy_verbs): {len(taxonomy_verbs)}\")\n",
    "print(f\"len(taxonomy_nouns): {len(taxonomy_nouns)}\")\n",
    "\n",
    "print(\n",
    "    \"taxonomy_verbs - struct_verb_counter.keys(): \"\n",
    "    f\"{taxonomy_verbs - struct_verb_counter.keys()}\"\n",
    ")\n",
    "print(\n",
    "    \"struct_verb_counter.keys() - taxonomy_verbs: \"\n",
    "    f\"{struct_verb_counter.keys() - taxonomy_verbs}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"taxonomy_nouns - struct_noun_counter.keys(): \"\n",
    "    f\"{taxonomy_nouns - struct_noun_counter.keys()}\"\n",
    ")\n",
    "print(\n",
    "    \"struct_noun_counter.keys() - taxonomy_nouns: \"\n",
    "    f\"{struct_noun_counter.keys() - taxonomy_nouns}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a bar chart for structured verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Define a function to truncate labels\n",
    "\n",
    "\n",
    "def truncate_label(label, max_length=10):\n",
    "    return label if len(label) <= max_length else label[:max_length] + \"...\"\n",
    "\n",
    "\n",
    "labels, values = zip(\n",
    "    *[\n",
    "        (verb, count)\n",
    "        for verb, count in struct_verb_counter.most_common()\n",
    "        if verb is not None\n",
    "    ]\n",
    ")\n",
    "truncated_labels = [truncate_label(label) for label in labels]\n",
    "\n",
    "# Calculate the cumulative sum and find the index where it crosses 80%\n",
    "cumulative_sum = np.cumsum(values)\n",
    "eighty_percent_index = np.where(cumulative_sum >= 0.8 * cumulative_sum[-1])[0][0]\n",
    "\n",
    "# Print the verbs that make up 80% and the rest\n",
    "print(\"Verbs making up 80% of samples:\", labels[: eighty_percent_index + 1])\n",
    "print(\"Remaining verbs:\", labels[eighty_percent_index + 1 :])\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "bars = plt.bar(truncated_labels, values)\n",
    "\n",
    "# Mark the 80% threshold on the bar chart\n",
    "plt.axvline(\n",
    "    x=eighty_percent_index + 0.5, color=\"red\", linestyle=\"--\", label=\"80% threshold\"\n",
    ")  # +0.5 to place line between bars\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Verbs\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Verb Count\")\n",
    "\n",
    "# Sparse labeling: Show every nth label\n",
    "n = 3  # adjust this based on your data and preferences\n",
    "sparse_labels = [\n",
    "    \"\" if i % n != 0 else label for i, label in enumerate(truncated_labels)\n",
    "]\n",
    "plt.xticks(range(len(labels)), sparse_labels, rotation=45, ha=\"right\", fontsize=10)\n",
    "\n",
    "# Adjust x-axis limits to remove margins\n",
    "plt.xlim(-0.5, len(labels) - 0.5)\n",
    "\n",
    "# Display the count on top of each bar\n",
    "for i, bar in enumerate(bars):\n",
    "    if i % n == 0:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            yval + 0.5,\n",
    "            str(yval),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()  # Adjust layout for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a bar chart for structured nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, values = zip(\n",
    "    *[\n",
    "        (noun, count)\n",
    "        for noun, count in struct_noun_counter.most_common()\n",
    "        if noun is not None\n",
    "    ]\n",
    ")\n",
    "truncated_labels = [truncate_label(label) for label in labels]\n",
    "\n",
    "# Calculate the cumulative sum and find the index where it crosses 80%\n",
    "cumulative_sum = np.cumsum(values)\n",
    "eighty_percent_index = np.where(cumulative_sum >= 0.8 * cumulative_sum[-1])[0][0]\n",
    "\n",
    "# Print the nouns that make up 80% and the rest\n",
    "print(\"Nouns making up 80% of samples:\", labels[: eighty_percent_index + 1])\n",
    "print(\"Remaining verbs:\", labels[eighty_percent_index + 1 :])\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "bars = plt.bar(truncated_labels, values)\n",
    "\n",
    "# Mark the 80% threshold on the bar chart\n",
    "plt.axvline(\n",
    "    x=eighty_percent_index + 0.5, color=\"red\", linestyle=\"--\", label=\"80% threshold\"\n",
    ")  # +0.5 to place line between bars\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Nouns\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Noun Count\")\n",
    "\n",
    "# Sparse labeling: Show every nth label\n",
    "n = 5  # adjust this based on your data and preferences\n",
    "sparse_labels = [\n",
    "    \"\" if i % n != 0 else label for i, label in enumerate(truncated_labels)\n",
    "]\n",
    "plt.xticks(range(len(labels)), sparse_labels, rotation=45, ha=\"right\", fontsize=10)\n",
    "\n",
    "# Adjust x-axis limits to remove margins\n",
    "plt.xlim(-0.5, len(labels) - 0.5)\n",
    "\n",
    "# Display the count on top of each bar\n",
    "for i, bar in enumerate(bars):\n",
    "    if i % n == 0:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            yval + 0.5,\n",
    "            str(yval),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()  # Adjust layout for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a bar chart for (structured_verb, structured_noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, values = zip(\n",
    "    *[(pair, count) for pair, count in struct_verb_noun_counter.most_common()]\n",
    ")\n",
    "truncated_labels = [\n",
    "    f\"({truncate_label(verb), truncate_label(noun)})\" for verb, noun in labels\n",
    "]\n",
    "\n",
    "# Calculate the cumulative sum and find the index where it crosses 80%\n",
    "cumulative_sum = np.cumsum(values)\n",
    "eighty_percent_index = np.where(cumulative_sum >= 0.8 * cumulative_sum[-1])[0][0]\n",
    "\n",
    "# Print the nouns that make up 80% and the rest\n",
    "print(\"(verb, noun)'s making up 80% of samples:\", labels[: eighty_percent_index + 1])\n",
    "print(\"Remaining (verb, noun)'s:\", labels[eighty_percent_index + 1 :])\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "bars = plt.bar(truncated_labels, values)\n",
    "\n",
    "# Mark the 80% threshold on the bar chart\n",
    "plt.axvline(\n",
    "    x=eighty_percent_index + 0.5, color=\"red\", linestyle=\"--\", label=\"80% threshold\"\n",
    ")  # +0.5 to place line between bars\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"(verb, noun)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"(verb, noun) Count\")\n",
    "\n",
    "# Sparse labeling: Show every nth label\n",
    "n = 200  # adjust this based on your data and preferences\n",
    "sparse_labels = [\n",
    "    \"\" if i % n != 0 else label for i, label in enumerate(truncated_labels)\n",
    "]\n",
    "plt.xticks(range(len(labels)), sparse_labels, rotation=45, ha=\"right\", fontsize=10)\n",
    "\n",
    "# Adjust x-axis limits to remove margins\n",
    "plt.xlim(-0.5, len(labels) - 0.5)\n",
    "\n",
    "# Display the count on top of each bar\n",
    "for i, bar in enumerate(bars):\n",
    "    if i % n == 0:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            yval + 0.5,\n",
    "            str(yval),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()  # Adjust layout for better visibility\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's flatten structured verbs and nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r\"^(.+)_\\((.+)\\)$\")\n",
    "\n",
    "\n",
    "def extract_words(s):\n",
    "    m = pattern.match(s)\n",
    "    if m is None:\n",
    "        extracted_words = [s]\n",
    "    else:\n",
    "        extracted_words = [m.group(1)] + m.group(2).split(\",_\")\n",
    "    words = []\n",
    "    for extracted in extracted_words:\n",
    "        words.extend(\n",
    "            word.replace(\"-\", \" \").replace(\"_\", \" \") for word in extracted.split(\"/\")\n",
    "        )\n",
    "    return words\n",
    "\n",
    "\n",
    "flat_verbs = set()\n",
    "for verb in taxonomy_verbs:\n",
    "    flat_verbs.update(extract_words(verb))\n",
    "print(f\"len(flat_verbs) = {len(flat_verbs)}\")\n",
    "print(\"Flat verbs:\")\n",
    "for verb in flat_verbs:\n",
    "    print(verb)\n",
    "\n",
    "flat_nouns = set()\n",
    "for noun in taxonomy_nouns:\n",
    "    flat_nouns.update(extract_words(noun))\n",
    "print(f\"len(flat_nouns) = {len(flat_nouns)}\")\n",
    "print(\"Flat nouns:\")\n",
    "for noun in flat_nouns:\n",
    "    print(noun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the verbs and nouns from EPIC-KITCHENS 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def format_verb(verb: str) -> str:\n",
    "    return verb.replace(\"-\", \" \")\n",
    "\n",
    "\n",
    "def format_noun(noun: str) -> str:\n",
    "    if \":\" in noun:\n",
    "        return noun.split(\":\")[-1]\n",
    "    else:\n",
    "        return noun\n",
    "\n",
    "\n",
    "ek_verbs = set()\n",
    "ek_nouns = set()\n",
    "with open(\"../../../EPIC-KITCHENS/annotations/EPIC_100_validation.csv\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        ek_verbs.add(format_verb(row[\"verb\"]))\n",
    "        ek_nouns.add(format_noun(row[\"noun\"]))\n",
    "\n",
    "print(f\"len(ek_verbs) = {len(ek_verbs)}\")\n",
    "print(\"EPIC-KITCHENS verbs:\")\n",
    "for verb in ek_verbs:\n",
    "    print(verb)\n",
    "\n",
    "print(f\"len(ek_nouns) = {len(ek_nouns)}\")\n",
    "print(\"EPIC-KITCHENS nouns:\")\n",
    "for noun in ek_nouns:\n",
    "    print(noun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare verbs and nouns from the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_intersection = flat_verbs.intersection(ek_verbs)\n",
    "noun_intersection = flat_nouns.intersection(ek_nouns)\n",
    "\n",
    "print(\"verb intersection\")\n",
    "for verb in verb_intersection:\n",
    "    print(verb)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"flat_verbs - ek_verbs\")\n",
    "for verb in flat_verbs - ek_verbs:\n",
    "    print(verb)\n",
    "\n",
    "print()\n",
    "print(\"ek_verbs - flat_verbs\")\n",
    "for verb in ek_verbs - flat_verbs:\n",
    "    print(verb)\n",
    "\n",
    "print()\n",
    "print(\"noun intersection\")\n",
    "for noun in noun_intersection:\n",
    "    print(noun)\n",
    "\n",
    "print()\n",
    "print(\"flat_nouns - ek_nouns\")\n",
    "for noun in flat_nouns - ek_nouns:\n",
    "    print(noun)\n",
    "\n",
    "print()\n",
    "print(\"ek_nouns - flat_nouns\")\n",
    "for noun in ek_nouns - flat_nouns:\n",
    "    print(noun)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-blip-IEa7WKva-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
