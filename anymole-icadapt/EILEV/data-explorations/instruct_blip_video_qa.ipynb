{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video QA with InstructBLIP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the clip for an arbitrary action from Ego4d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "\n",
    "with open(\"../../ego4d/v2/annotations/fho_main.json\") as f:\n",
    "    fho_main = json.load(f)\n",
    "\n",
    "video = fho_main[\"videos\"][20]\n",
    "interval = video[\"annotated_intervals\"][2]\n",
    "action = interval[\"narrated_actions\"][4]\n",
    "\n",
    "print(f'video_uid: {video[\"video_uid\"]}')\n",
    "print(f'start_sec: {action[\"start_sec\"]}')\n",
    "print(f'end_sec: {action[\"end_sec\"]}')\n",
    "print(f'clip_uid: {interval[\"clip_uid\"]}')\n",
    "print(f'clip_start_sec: {action[\"clip_start_sec\"]}')\n",
    "print(f'clip_end_sec: {action[\"clip_end_sec\"]}')\n",
    "print(f'narration_text: {action[\"narration_text\"]}')\n",
    "\n",
    "video_path_handler = VideoPathHandler()\n",
    "video = video_path_handler.video_from_path(\n",
    "    f\"../../ego4d/v2/clips/{interval['clip_uid']}.mp4\"\n",
    ")\n",
    "clip = video.get_clip(action[\"clip_start_sec\"], action[\"clip_end_sec\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `blip2_vicuna_instruct:vicuna7b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lavis.common.registry import registry\n",
    "from lavis.models import load_preprocess\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "def load_lavis_model_and_preprocess(\n",
    "    name: str, model_type: str, is_eval: bool = False, device: str = \"cpu\", **kwargs\n",
    "):\n",
    "    model_cls = registry.get_model_class(name)\n",
    "    cfg = OmegaConf.load(model_cls.default_config_path(model_type))\n",
    "    model_cfg = cfg.model\n",
    "    model_cfg.update(**kwargs)\n",
    "    model = model_cls.from_config(model_cfg)\n",
    "    if is_eval:\n",
    "        model.eval()\n",
    "    if device == \"cpu\" or device == torch.device(\"cpu\"):\n",
    "        model = model.float()\n",
    "    model = model.to(device)\n",
    "\n",
    "    vis_processors, txt_processors = load_preprocess(cfg.preprocess)\n",
    "\n",
    "    # HACK: delete ToTensor() transform b/c VideoPathHandler already gives us\n",
    "    # tensors.\n",
    "    for _, vis_processor in vis_processors.items():\n",
    "        del vis_processor.transform.transforms[-2]\n",
    "\n",
    "    return model, vis_processors, txt_processors\n",
    "\n",
    "\n",
    "model, vis_processors, _ = load_lavis_model_and_preprocess(\n",
    "    \"blip2_vicuna_instruct\",\n",
    "    \"vicuna7b\",\n",
    "    is_eval=True,\n",
    "    device=\"cuda\",\n",
    "    llm_model=\"/path/to/vicuna-7b-v1.1\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the video and show as a gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_file_name):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    iio.imwrite(\n",
    "        gif_file_name,\n",
    "        video_tensor.permute(0, 2, 3, 1).numpy().astype(np.uint8),\n",
    "        extension=\".gif\",\n",
    "        # infinite loop\n",
    "        loop=0,\n",
    "    )\n",
    "    html = f'<img src=\"{gif_file_name}\" />'\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "frames = clip[\"video\"][:, ::30, ...]\n",
    "channel, time, _, _ = frames.size()\n",
    "frames = frames.permute(1, 0, 2, 3)\n",
    "frames = vis_processors[\"eval\"](frames)\n",
    "_, _, height, weight = frames.size()\n",
    "frames = frames.view(time, channel, height, weight)\n",
    "display_gif(frames, \"vicuna_frames.gif\")\n",
    "frames = frames.permute(1, 0, 2, 3).unsqueeze(0).to(model.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform video QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_txt = model.generate(\n",
    "    {\"image\": frames, \"prompt\": \"What is the camera wearer doing?\"}\n",
    ")[0]\n",
    "print(generated_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-blip-IEa7WKva-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
